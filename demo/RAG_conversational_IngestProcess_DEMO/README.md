This program processes PDF documents from a specified location, stores them in Cloudera Semantic Search (CSS) with vector embeddings, and allows users to query the data using a pre-trained language model. CSS handles the search and retrieval process, while Azure OpenAI generates responses based on the retrieved context. The embeddings are generated by an externally hosted model in Cloudera Machine Learning (CML) inferencing engine. CSS Machine Leanearing Tool agent is used to integrate vector search for context and interacting with LLM in Cloudera AI Inrerecing for answers. In this demo only user query is sent to the CSS and through the ML Tool infrastructure , CSS fetches the vectors semantically close to question that was asked, fetches the context and sends the prompt to the LLM hosted externally with out having to do any processing at the client site. For the subsequent questions, it uses previous history with CSS memory API and sends the question along with history to the LLM. This demo also shows text chunking processor capability in Ingest pipleine. In summary all 3 activities of Text chunnking, Embedding generation and LLM query is happening with in CSS context. 

- **This demo needs nodes having following roles**
    - data
    - ingest
    - ml


## Requirements
- **Python 3.8+**
- **Cloudera Semantic Search**  server with endpoints 
- **Azure OpenAI** credentials for query processing with the LLM
- **Cloudera AI** Embedding Model endpoint with CDP Token to connect to 
- **CSS Config** Need to disable plugin security by setting `plugins.security.disabled: true` in `opensearch.yml`. This demo currently runls only with 2.17.x version 

## Environment Variables
Set the following environment variables in your shell or `.env` file before running the program.

- **CSS Connection**  
  - `CSS_HOST`: Host address of the Cloudera Semantic Search server (default: `localhost`)
  - `CSS_PORT`: Port of the OpenSearch server (default: `9200`)
  - `CSS_USER`: Username for OpenSearch authentication
  - `CSS_PASSWORD`: Password for OpenSearch authentication
  - `DATA_FILE_PATH`: PDF files to load (default: `./data`)
  - `CSS_CDP_TOKEN_KEY`: CDP Token for the environment.
  - `CSS_CML_LLM_MODEL` : CAII  model for LLM Chat
  - `CSS_CML_LLM_ENDPOINT` : OpenAI model servicing Endpoint 
  - `CSS_SSL`:  True if SSL is enabled for CSS connection 
  - `CSS_EMBEDDING_OPENAI_MODEL`: pre-trained and supported Embedding model in CSS  ,
  - `CSS_EMBEDDING_DIMENSION` : Dimension of the vector embedding generated by the CML Model. 
  

2. **Install Dependencies**:
   Install the necessary packages by running:
   ```bash
   pip3 install -r session-install-deps/requirements.txt
   ```

## Running the Program
1. **Start the OpenSearch Server**:
   Ensure your Cloudera Semantic search  server is running and accessible at the host and port specified in the environment variables.

2. **Run the Script**:
   Start the following job for loading the data:
   ```python
   python css_load.py 
   ```
   This will:
   - Connect to CSS
   - Creates the connector to externally hosted CAII LLM   model 
   - Registers and deploys the emberdding and inferencing model with connector ID for  Inferencing model.
   - Registers the ML Tool referencing  embedding and inferencing model for RAG use case. 
   - Creates the neural search pipleine referencing Embedding model for generating embeddings.
   - Creates index referring the neural search pipeline
   - Extract text from  PDF documents from the specified directory. Currently there is one Cloudera Operationa Database document in the PDF format. One can add more to the same directory.
   - Ingest the text through neural search pipleine into the CSS vector store. Pipeline will chunk the text, generate embedding and insert the chunk and embedding into the index. 
   - Tests the query feature with a hard coded query invokign ML Tool for RAG

   Start the following Application for bringing up the search UI:
   ```python
   python search_app.py 
   ```
   If it is run as CML AMP , then you can use <subdomain.cml-wkspace> to access the app.

    RUn  the following Application to do cleanup of index , neural pipeline and model:
   ```python
   python clenaup.py 
   ```
   This will:
   - Connect to CSS
   - Deletes index, neural pipeline, undeploys and deletes the embedding models, connectors and ML Tool

## Usage
- **Querying the System**:
   - The Gradio UI provides an interface for users to enter a query. Upon submission, it:
     1. Converts the query into a vector and searches for similar document chunks in CSS through neural search feature of CSS.
     2. Feeds the retrieved context to Cloudera AII to generate an answer.
     4. Displays Index settings, mappings and neural pipeline definitions used.
     5. Displays the Embedding , inferencing Models.
     6. Display connector for externally hosted models.
     7. Display the ML Tool definition.
     8. Display memory_id of the history.





