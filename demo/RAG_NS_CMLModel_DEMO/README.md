This program processes PDF documents from a specified location, stores them in Cloudera Semantic Search (CSS) with vector embeddings, and allows users to query the data using a pre-trained language model. CSS handles the search and retrieval process, while Azure OpenAI generates responses based on the retrieved context. The embeddings are generated by an externally hosted model in Cloudera Machine Learning (CML) inferencing engine. This external model is  used in  Neural search pipeline feature for generating embeddings. 

- **This demo needs nodes having following roles**
    - data
    - ingest
    - ml


## Requirements
- **Python 3.8+**
- **Cloudera Semantic Search  server with endpoints 
- **Azure OpenAI** credentials for query processing with the LLM

## Environment Variables
Set the following environment variables in your shell or `.env` file before running the program.

- **CSS Connection**  
  - `CSS_HOST`: Host address of the Cloudera Semantic Search server (default: `localhost`)
  - `CSS_PORT`: Port of the OpenSearch server (default: `9200`)
  - `CSS_USER`: Username for OpenSearch authentication
  - `CSS_PASSWORD`: Password for OpenSearch authentication
  - `DATA_FILE_PATH`: PDF files to load (default: `./data`)
  - `CSS_OPENAI_KEY`: Azure OpenAI key 
  - `CSS_OPENAI_VERSION` : OpenAI model version
  - `CSS_OPENAI_MODEL` : OpenAI model for LLM Chat
  - `CSS_OPENAI_ENDPOINT` : OpenAI model servicing Endpoint
  - `CSS_SSL`:  True if SSL is enabled for CSS connection
  - `CSS_EMBEDDING_OPENAI_KEY`:  This is the CDP_TOKEN obtained through cdp cli for the enviroment running CML model 
  - `CSS_EMBEDDING_OPENAI_ENDPOINT`: CML Model endpoint URL obtained from CML Model UI for generating embeddings,
  - `CSS_EMBEDDING_OPENAI_MODEL`: Model id of the CML Model deployed,
  - `CSS_EMBEDDING_DIMENSION` : Dimension of the vector embedding generated by the CML Model. 
  

2. **Install Dependencies**:
   Install the necessary packages by running:
   ```bash
   pip3 install -r session-install-deps/requirements.txt
   ```

## Running the Program
1. **Start the OpenSearch Server**:
   Ensure your Cloudera Semantic search  server is running and accessible at the host and port specified in the environment variables.

2. **Run the Script**:
   Start the following job for loading the data:
   ```python
   python css_load.py 
   ```
   This will:
   - Connect to CSS
   - Creates the conntector to externally hosted CML model serving endpoint
   - Registers and deploys the inferencing model with connector ID for generating embedding.
   - Creates the neural search pipleine referencing inferencing model for genering embeddings.
   - Creates index refering the neural search pipeline
   - Process and Chunk the  PDF documents from the specified directory. Currently there is one Cloudera Operationa Database document in the PDF format. One can add more to the same directory.
   - Ingest the chunks through neural search pipleine into the CSS vector store
   - Tests the query feature with a hard coded query

   Start the following Application for bringing up the search UI:
   ```python
   python search_app.py 
   ```

    RUn  the following Application to do cleanup of index , neural pipeline and model:
   ```python
   python clenaup.py 
   ```
   This will:
   - Connect to CSS
   - Deletes index, neural pipeline, undeploys and deletes the embedding model.

## Usage
- **Querying the System**:
   - The Gradio UI provides an interface for users to enter a query. Upon submission, it:
     1. Converts the query into a vector and searches for similar document chunks in CSS through neural search feature of CSS.
     2. Feeds the retrieved context to Azure OpenAI to generate an answer.
     3. Displays the answer along with chunks  of  the original document chunks.
     4. Displays Index settings, mappings and neural pipeline definitions used.





